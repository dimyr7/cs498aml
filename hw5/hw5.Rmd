---
output:
  html_document:
    theme: readable
    toc: yes
---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
options(scipen=1, digits=4, width=80)
library(faraway)
library(readr)
library(caret)
library(klaR)
library(lattice)
library(glmnet)
library(plotmo)
```

# CS 498 AML Homework 5
## Question 1 - Music
```{r, message=FALSE}
music <- read_csv('/Users/brunocalogero/Desktop/UIUC/cs498aml/cs498aml/hw5/music.csv')
num_features = dim(music)[2]-2
music["latitude"] <- music["latitude"] + 90
music["longitude"] <- music["longitude"] + 180
```
A small angle transformation was required here so as to allow us to use the box-cox transformation correctly. 

### Latitude
**(a)** Fit a linear model

```{r}
lat.lm <- lm(latitude ~ . - longitude, data = music)
```

```{r, echo=FALSE}
print(c("The R^2 value is", summary(lat.lm)$r.squared))
xyplot(lat.lm$residuals ~ lat.lm$fitted,
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Predicting Latitude",
	  sub  = "Original Scale",
	  panel = function(x, y, ...)
      {
		panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```
A natural measure of the goodness of a regression is what percentage of the variance of y it explains. Essentially, the Rsquared value tells us how well the regression explains the training data. Good predictions result in high values of Rsquared, and a perfect model will have Rsquared = 1 (which doesn’t usually happen). In this case our Rsquared estimation is rather low (approx. 0.29) , explicitely telling us that we should consider a better model or perhaps a transformation. Indeed, The Box-Cox transformation is a method that can search for a transformation of the dependent variable that improves the regression.The method uses a one-parameter family of transformations, with parameter λ, then searches for the best value of this parameter using maximum likelihood. One searches for a value of λ that makes residuals look most like a normal distribution.

**(b)** Box-Cox

```{r}
lat.bc_info = boxcox(lat.lm, lambda=seq(-1,5))
lat.bc_lambda = lat.bc_info$x[which.max(lat.bc_info$y)] #selecting best lambda
#we obtain a best lambda of around 3.5454 with boxcox 
lat.lm_lambda = lm(latitude^lat.bc_lambda ~ . - longitude, data=music)
# now let us see if we have improved our model and Rsquared value
```

```{r, echo=FALSE}
print(c("The R^2 value is", summary(lat.lm_lambda)$r.squared))
print(c("The Box-Cox optimal lambda is", lat.bc_lambda))
xyplot(lat.lm_lambda$residuals ~ lat.lm_lambda$fitted,
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Predicting Latitude",
      sub = toString(c("power", lat.bc_lambda)),
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```

The Box-Cox transformation helps with the lattitude. This is evident because the residuals have dropped from about $30$ to $10^{-7}$
Moreover, we have a new Rsquared value that has increased to approx. 0.32 which clearly demonstrates an improvement from the original 0.29. Hence we will be using the box-cox model for the rest of the excercise. We now proced to regularization, and more specifically Ridge and Lasso regularizations, we want to see if we can still improve things at this point. 

**(c)** Regularization
```{r}
# Ridge L2
lat.l2 = glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=0)
lat.l2_cv = cv.glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=0)
plotres(lat.l2)
par(mfrow=c(1,2))
plot(lat.l2, sub="Coef L2")
plot(lat.l2_cv, sub="Error L2")
```

```{r}
# Lasso L1
lat.l1 = glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=1)
lat.l1_cv = cv.glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["latitude"]^lat.bc_lambda),
				alpha=1)
plotres(lat.l1)
par(mfrow=c(1,2))
plot(lat.l1, sub="coef L1")
plot(lat.l1_cv, sub="Error L1")
```



### Longitude
**(a)** Fit a linear model
```{r}
lon.lm <- lm(longitude ~ . - latitude, data = music)
```

```{r, echo=FALSE}
print(c("The R^2 value is", summary(lon.lm)$r.squared))
xyplot(lon.lm$residuals ~ lon.lm$fitted,
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Predicting Longitude",
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```

**(b)** Box-Cox

```{r}
lon.bc_info = boxcox(lon.lm)
lon.bc_lambda = lon.bc_info$x[which.max(lon.bc_info$y)]
lon.lm_lambda = lm(longitude^lon.bc_lambda ~ . - latitude, data=music)
```

```{r, echo=FALSE}
print(c("The R^2 value is", summary(lon.lm_lambda)$r.squared))
print(c("The Box-Cox optimal lambda is", lon.bc_lambda))
xyplot(lon.lm_lambda$residuals ~ lon.lm_lambda$fitted,
      xlab = "Fitted Values",
      ylab = "Residuals",
      main = "Predicting Longitude",
      sub = toString(c("power", lon.bc_lambda)),
      panel = function(x, y, ...)
      {
        panel.grid(h = -1, v = -1)
        panel.abline(h = 0)
        panel.xyplot(x, y, ...)
      }
)
```

The Box-Cox transformation does not help becuase the residuals are not really effected. Since the optimal $\lambda$ found by Box-Cox is about $1$, we don't expect this to do a lot.

**(c)** Regularization

```{r}
# Ridge L2
lat.l2 = glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["longitude"]^lat.bc_lambda),
				alpha=0)
lat.l2_cv = cv.glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["longitude"]^lat.bc_lambda),
				alpha=0)

print(c("the minimum  lambda is :", log(lat.l1_cv$lambda.min)))

plotres(lat.l2)
par(mfrow=c(1,2))
plot(lat.l2, sub="Coef L2")
plot(lat.l2_cv, sub="Error L2")
```

```{r}
# Lasso L1
lat.l1 = glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["longitude"]^lat.bc_lambda),
				alpha=1)
lat.l1_cv = cv.glmnet(as.matrix(music[1:num_features]),
				as.matrix(music["longitude"]^lat.bc_lambda),
				alpha=1)

print(c("the minimum  lambda is :", log(lat.l1_cv$lambda.min)))

plotres(lat.l1)
par(mfrow=c(1,2))
plot(lat.l1, sub="coef L1")
plot(lat.l1_cv, sub="Error L1")
```

```{r}


plot(log(lat.l1_cv$lambda),lat.l1_cv$cvm,pch=19,col="red",xlab="log(Lambda)",ylab=lat.l1_cv$name)
points(log(lat.l2_cv$lambda),lat.l2_cv$cvm,pch=19,col="grey")
legend("topleft",legend=c("alpha= 1 L1","alpha= 1 L2"),pch=19,col=c("red","grey"))


```


## Question 2 - Taiwan

## Question 3 - Cancer
